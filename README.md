# Llama-Index
Testing Llama Index Evaluation Tools and providing insights regarding performance.


Environemnt: Local 
Model: Ollama ( llama3.1)
Non Rag and Rag Both
Installations:

1. Install Ollama first from browser.
2. Run "ollama pull llama3.1/or any model" in powershell
3. run "pip install streamlit llama-index pandas"
4. Run any module.



Interface.py
Streamlit interface to enter query and get raw response from llm and evalutaion values

Context-Passsing.py
A specific command line based Module where we are running test case 1.

No-Context.py
A specific command line based Module where we are running test case 2.
